{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32179331",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f2b705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tạo xong 1892 file JSON tại D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset\\dev\n",
      "Đã tạo xong 2000 file JSON tại D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset\\train\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "files = ['dev', 'train']\n",
    "\n",
    "# Đường dẫn đến file JSON lớn của bạn\n",
    "for file in files: \n",
    "    input_file = rf'D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset\\{file}.json'\n",
    "    # Thư mục DATA bạn đã thiết lập\n",
    "    output_dir = rf'D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset\\{file}'\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        big_data = json.load(f)\n",
    "\n",
    "    # Lặp qua từng bài viết và lưu thành file riêng lẻ\n",
    "    for i, (key, value) in enumerate(big_data.items()):\n",
    "        # Chuyển đổi cấu trúc source từ dict {\"0\": [...]} thành list [...]\n",
    "        article_sents = value['source']['0']\n",
    "        abstract_text = value['target']\n",
    "        \n",
    "        # Tạo object mới đúng cấu trúc script mong đợi\n",
    "        single_data = {\n",
    "            \"source\": article_sents,  # Script của bạn đang dùng key 'source'\n",
    "            \"target\": [abstract_text] # Đưa target vào list vì script tokenize theo list\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(output_dir, f'{i}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(single_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Đã tạo xong {len(big_data)} file JSON tại {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa70c3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tạo xong 3789 file JSON tại D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset\\test\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "files = ['test']\n",
    "\n",
    "# Đường dẫn đến file JSON lớn của bạn\n",
    "for file in files: \n",
    "    input_file = rf'D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset\\{file}.json'\n",
    "    # Thư mục DATA bạn đã thiết lập\n",
    "    output_dir = rf'D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset\\{file}'\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        big_data = json.load(f)\n",
    "\n",
    "    # Lặp qua từng bài viết và lưu thành file riêng lẻ\n",
    "    for i, (key, value) in enumerate(big_data.items()):\n",
    "        # Chuyển đổi cấu trúc source từ dict {\"0\": [...]} thành list [...]\n",
    "        article_sents = value['source']['0']\n",
    "        abstract_text = value['target']\n",
    "        \n",
    "        # Tạo object mới đúng cấu trúc script mong đợi\n",
    "        single_data = {\n",
    "            \"source\": article_sents,  # Script của bạn đang dùng key 'source'\n",
    "            \"target\": [abstract_text] # Đưa target vào list vì script tokenize theo list\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(output_dir, f'{i}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(single_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Đã tạo xong {len(big_data)} file JSON tại {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22ecbe",
   "metadata": {},
   "source": [
    "# vocab_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a25a6d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang quét dữ liệu để tạo vocab_cnt.pkl...\n",
      "Hoàn thành! Đã tạo file tại: D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset\\vocab_cnt.pkl\n",
      "Tổng số từ vựng: 13335\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from os.path import join\n",
    "\n",
    "# Trỏ đến thư mục dataset của bạn\n",
    "DATA_DIR = r'D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset'\n",
    "\n",
    "def build_vocab():\n",
    "    word_cnt = Counter()\n",
    "    # Chỉ nên quét tập train để xây dựng từ điển\n",
    "    split_dir = join(DATA_DIR, 'train')\n",
    "    \n",
    "    if not os.path.exists(split_dir):\n",
    "        print(f\"Lỗi: Không tìm thấy thư mục {split_dir}\")\n",
    "        return\n",
    "\n",
    "    print(\"Đang quét dữ liệu để tạo vocab_cnt.pkl...\")\n",
    "    files = [f for f in os.listdir(split_dir) if f.endswith('.json')]\n",
    "    \n",
    "    for filename in files:\n",
    "        with open(join(split_dir, filename), 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            # Theo cấu trúc Wikilingual bạn đã gửi:\n",
    "            # data['source'] là list các câu, data['target'] là string hoặc list\n",
    "            articles = data.get('source', [])\n",
    "            abstracts = data.get('target', [])\n",
    "            if isinstance(abstracts, str): abstracts = [abstracts]\n",
    "            \n",
    "            for sent in articles + abstracts:\n",
    "                word_cnt.update(sent.lower().split())\n",
    "\n",
    "    # Lưu file pkl\n",
    "    with open(join(DATA_DIR, 'vocab_cnt.pkl'), 'wb') as f:\n",
    "        pickle.dump(word_cnt, f)\n",
    "    \n",
    "    print(f\"Hoàn thành! Đã tạo file tại: {join(DATA_DIR, 'vocab_cnt.pkl')}\")\n",
    "    print(f\"Tổng số từ vựng: {len(word_cnt)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "078e4f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xử lý tập train (Tách từ đơn)...\n",
      "Đang xử lý tập dev (Tách từ đơn)...\n",
      "Đang xử lý tập test (Tách từ đơn)...\n",
      "--- Đã tạo xong vocab_cnt.pkl. Tổng từ: 17994 ---\n",
      "Đang đóng gói D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset\\train.tar...\n",
      "Đang đóng gói D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset\\dev.tar...\n",
      "Đang đóng gói D:\\NguyenTienDat_23520262\\Nam_3\\ViPhoLM_DoAn\\datasets\\Wikilingual-dataset\\test.tar...\n",
      "--- HOÀN TẤT QUY TRÌNH TIỀN XỬ LÝ ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import tarfile\n",
    "import re\n",
    "from collections import Counter\n",
    "from os.path import join\n",
    "import unicodedata\n",
    "\n",
    "# Đường dẫn thư mục dữ liệu trên Linux của bạn\n",
    "DATA_DIR = \"D:\\\\NguyenTienDat_23520262\\\\Nam_3\\\\ViPhoLM_DoAn\\\\datasets\\\\Wikilingual-dataset\"\n",
    "\n",
    "def simple_tokenize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = unicodedata.normalize(\"NFC\", sentence)\n",
    "    # remove all non-characters and punctuations\n",
    "    sentence = re.sub(r\"[-/“”!\\*\\&\\$\\.\\?:;,\\\"'\\(\\[\\]\\(\\)]\", \" \", sentence)\n",
    "    sentence = \" \".join(sentence.strip().split()) # remove duplicated spaces\n",
    "    tokens = sentence.strip().split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def process_and_build():\n",
    "    word_cnt = Counter()\n",
    "    splits = ['train', 'dev', 'test']\n",
    "    \n",
    "    for split in splits:\n",
    "        input_file = join(DATA_DIR, f\"{split}.json\")\n",
    "        output_dir = join(DATA_DIR, split)\n",
    "        if not os.path.exists(output_dir): \n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        print(f\"Đang xử lý tập {split} (Tách từ đơn)...\")\n",
    "        \n",
    "        # Kiểm tra file input\n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"Bỏ qua {split} vì không tìm thấy file .json\")\n",
    "            continue\n",
    "\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            big_data = json.load(f)\n",
    "            \n",
    "        for i, (key, value) in enumerate(big_data.items()):\n",
    "            # Lấy dữ liệu nguồn và mục tiêu\n",
    "            # Giả sử cấu trúc: source: {\"0\": [\"câu 1\", \"câu 2\"]}, target: \"chuỗi tóm tắt\"\n",
    "            raw_source = value['source'].get('0', []) if isinstance(value['source'], dict) else value['source']\n",
    "            raw_target = value['target']\n",
    "\n",
    "            # Tách thành từng từ đơn\n",
    "            source_sents = [\" \".join(simple_tokenize(s)) for s in raw_source]\n",
    "            # Target phải để trong List theo yêu cầu của repo fast_abs_rl\n",
    "            target_text = [\" \".join(simple_tokenize(raw_target))]\n",
    "            \n",
    "            # Cập nhật từ điển Vocab (tách nhỏ ra để đếm từng từ)\n",
    "            for sent in source_sents + target_text:\n",
    "                word_cnt.update(sent.split())\n",
    "            \n",
    "            # Lưu file JSON nhỏ lẻ\n",
    "            single_data = {\n",
    "                \"article\": source_sents, \n",
    "                \"abstract\": target_text\n",
    "            }\n",
    "            with open(join(output_dir, f'{i}.json'), 'w', encoding='utf-8') as f_out:\n",
    "                json.dump(single_data, f_out, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Lưu file vocab_cnt.pkl (đây là file Counter đối tượng)\n",
    "    with open(join(DATA_DIR, 'vocab_cnt.pkl'), 'wb') as f_vocab:\n",
    "        pickle.dump(word_cnt, f_vocab)\n",
    "    print(f\"--- Đã tạo xong vocab_cnt.pkl. Tổng từ: {len(word_cnt)} ---\")\n",
    "\n",
    "    # Đóng gói thành Tarballs (.tar)\n",
    "    for split in splits:\n",
    "        tar_path = join(DATA_DIR, f\"{split}.tar\")\n",
    "        print(f\"Đang đóng gói {tar_path}...\")\n",
    "        with tarfile.open(tar_path, \"w\") as tar:\n",
    "            # arcname=split giúp cấu trúc bên trong file tar gọn gàng hơn\n",
    "            tar.add(join(DATA_DIR, split), arcname=split)\n",
    "            \n",
    "    print(\"--- HOÀN TẤT QUY TRÌNH TIỀN XỬ LÝ ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_and_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f285c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: METEOR is not configured\n",
      "start processing dev split...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"produce the dataset with (psudo) extraction label\"\"\"\n",
    "import os\n",
    "from os.path import exists, join\n",
    "import json\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import multiprocessing as mp\n",
    "\n",
    "from cytoolz import curry, compose\n",
    "\n",
    "from utils import count_data\n",
    "from metric import compute_rouge_l\n",
    "\n",
    "try:\n",
    "    OG_DATA_DIR = \"D:\\\\GitHub_Repo\\\\ViPhoLM\\\\models\\\\fast_abstractive\\\\fast_abs_rl\\\\data\"\n",
    "except KeyError:\n",
    "    print('please use environment variable to specify data directories')\n",
    "\n",
    "try:\n",
    "    OF_DATA_DIR = \"D:\\\\GitHub_Repo\\\\ViPhoLM\\\\models\\\\fast_abstractive\\\\fast_abs_rl\\\\data\"\n",
    "except KeyError:\n",
    "    print('please use environment variable to specify data directories')\n",
    "\n",
    "def _split_words(texts):\n",
    "    return map(lambda t: t.split(), texts)\n",
    "\n",
    "\n",
    "def get_extract_label(art_sents, abs_sents):\n",
    "    \"\"\" greedily match summary sentences to article sentences\"\"\"\n",
    "    extracted = []\n",
    "    scores = []\n",
    "    indices = list(range(len(art_sents)))\n",
    "    for abst in abs_sents:\n",
    "        rouges = list(map(compute_rouge_l(reference=abst, mode='r'),\n",
    "                          art_sents))\n",
    "        ext = max(indices, key=lambda i: rouges[i])\n",
    "        indices.remove(ext)\n",
    "        extracted.append(ext)\n",
    "        scores.append(rouges[ext])\n",
    "        if not indices:\n",
    "            break\n",
    "    return extracted, scores\n",
    "\n",
    "@curry\n",
    "def process(split, i):\n",
    "    data_dir = join(OG_DATA_DIR, split)\n",
    "    with open(join(data_dir, '{}.json'.format(i))) as f:\n",
    "        data = json.loads(f.read())\n",
    "    tokenize = compose(list, _split_words)\n",
    "    art_sents = tokenize(data['source'])\n",
    "    abs_sents = tokenize(data['target'])\n",
    "    if art_sents and abs_sents: # some data contains empty article/abstract\n",
    "        extracted, scores = get_extract_label(art_sents, abs_sents)\n",
    "    else:\n",
    "        extracted, scores = [], []\n",
    "    data['extracted'] = extracted\n",
    "    data['score'] = scores\n",
    "    with open(join(data_dir, '{}.json'.format(i)), 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "def label_mp(split):\n",
    "    \"\"\" process the data split with multi-processing\"\"\"\n",
    "    start = time()\n",
    "    print('start processing {} split...'.format(split))\n",
    "    data_dir = join(OG_DATA_DIR, split)\n",
    "    n_data = count_data(data_dir)\n",
    "    with mp.Pool() as pool:\n",
    "        list(pool.imap_unordered(process(split),\n",
    "                                 list(range(n_data)), chunksize=1024))\n",
    "    print('finished in {}'.format(timedelta(seconds=time()-start)))\n",
    "\n",
    "def label(split):\n",
    "    start = time()\n",
    "    print('start processing {} split...'.format(split))\n",
    "    data_dir = join(OF_DATA_DIR, split)\n",
    "    n_data = count_data(data_dir)\n",
    "    for i in range(n_data):\n",
    "        print('processing {}/{} ({:.2f}%%)\\r'.format(i, n_data, 100*i/n_data),\n",
    "              end='')\n",
    "        with open(join(data_dir, '{}.json'.format(i))) as f:\n",
    "            data = json.loads(f.read())\n",
    "        tokenize = compose(list, _split_words)\n",
    "        art_sents = tokenize(data['article'])\n",
    "        abs_sents = tokenize(data['abstract'])\n",
    "        extracted, scores = get_extract_label(art_sents, abs_sents)\n",
    "        data['extracted'] = extracted\n",
    "        data['score'] = scores\n",
    "        with open(join(data_dir, '{}.json'.format(i)), 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "    print('finished in {}'.format(timedelta(seconds=time()-start)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    for split in ['dev', 'train']:  # no need of extraction label when testing\n",
    "        label_mp(split)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
