
vocab:
  type: Vocab
  path:
    train: datasets/VietNews-dataset/train.json
    dev: datasets/VietNews-dataset/dev.json
    test: datasets/VietNews-dataset/test.json
  min_freq: 3
  bos_token: <bos>
  eos_token: <eos>
  unk_token: <unk>
  pad_token: <pad>

dataset:
  train: 
    type: TextSumDataset
    path: datasets/VietNews-dataset/train.json
  dev:
    type: TextSumDataset
    path: datasets/VietNews-dataset/dev.json
  test: 
    type: TextSumDataset
    path: datasets/VietNews-dataset/test.json
  batch_size: 4
  num_workers: 2

model:
  name: Seneca_VietNews
  architecture: SENECAModel
  d_model: 256  # kích thước tổng thể của embedding model

  SENECA:
    EXTRACTOR:
      EMB_DIM: 256
      CONV_HIDDEN: 256
      LSTM_HIDDEN: 256
      LSTM_LAYER: 1
      BIDIRECTIONAL: True
      N_HOP: 2
      DROPOUT: 0.2

    GENERATOR:
      EMB_DIM: 256
      N_HIDDEN: 256
      BIDIRECTIONAL: False
      N_LAYER: 2
      DROPOUT: 0.2
  INFERENCE:
    TOP_K: 5              # số câu trích xuất từ extractor
    BEAM_SIZE: 2
    MAX_LEN: 90
    MIN_LEN: 20
    MAX_ART_LEN: 800      # giới hạn độ dài input cho generator
    DIVERSE: 0.5          # diversity penalty trong beam search

  device: cuda  # đổi sang GPU nếu dùng Colab

training:
  batch_size: 8
  learning_rate: 0.0005     # LR chuẩn cho SENECA
  extractor_loss_weight: 0.1  # đảm bảo extractor không dominate
  generator_loss_weight: 0.9  # nâng trọng số generator
  checkpoint_path: checkpoints
  epochs: 20
  rl_lr: 0.00001
  rl_weight: 0.99
  warmup: 400               # warmup ngắn hơn
  patience: 4
  grad_clip: 1.0            # giảm độ nhạy
  score: rouge-L

task: TextSumTask