vocab:
  type: Vocab
  path:
    train: /kaggle/input/wikilingual-test-2/datasets/datasets/Wikilingual-dataset/train.json
    dev: /kaggle/input/wikilingual-test-2/datasets/datasets/Wikilingual-dataset/dev.json
    test: /kaggle/input/wikilingual-test-2/datasets/datasets/Wikilingual-dataset/test.json
  min_freq: 3
  bos_token: <bos>
  eos_token: <eos>
  unk_token: <unk>
  pad_token: <pad>

dataset:
  train:
    type: TextSumDatasetOOV
    path: /kaggle/input/wikilingual-test-2/datasets/datasets/Wikilingual-dataset/train.json
  dev:
    type: TextSumDatasetOOV
    path: /kaggle/input/wikilingual-test-2/datasets/datasets/Wikilingual-dataset/dev.json
  test:
    type: TextSumDatasetOOV
    path: /kaggle/input/wikilingual-test-2/datasets/datasets/Wikilingual-dataset/test.json
  batch_size: 8
  num_workers: 4

model:
  name: Closedbook_Wikilingual
  architecture: ClosedbookSummarization
  d_model: 256
  embedding_size: 128
  checkpoint: checkpoints/Closedbook_Wikilingual_model.pt
  hidden_size: 256
  num_enc_layers: 1
  num_dec_layers: 1
  bidirectional: true
  is_attention: true
  is_attention_closedbook: false
  is_pgen: true
  is_coverage: true
  cov_loss_lambda: 1.0
  device: cuda
  dropout_ratio: 0.1
training:
  checkpoint_path: "checkpoints"
  learning_rate: 0.15
  score: rouge-L  # Evaluation metric
  patience: 3
  warmup: 1000
  epochs: 10
  max_iterations: 500000

task: TextSumTaskOOV