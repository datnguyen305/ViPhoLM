vocab:
  type: Vocab
  path:
    train: datasets/Wikilingual-dataset/train.json
    dev: datasets/Wikilingual-dataset/dev.json
    test: datasets/Wikilingual-dataset/test.json
  min_freq: 3
  bos_token: <bos>
  eos_token: <eos>
  unk_token: <unk>
  pad_token: <pad>
  max_sentence_length: 512

dataset:
  train:
    type: TextSumDataset
    path: datasets/Wikilingual-dataset/train.json
  dev:
    type: TextSumDataset
    path: datasets/Wikilingual-dataset/dev.json
  test:
    type: TextSumDataset
    path: datasets/Wikilingual-dataset/test.json
  batch_size: 32
  num_workers: 4`


model:
  name: closedbook_Wikilingual
  architecture: ClosedBookModel
  d_model: 512
  checkpoint: checkpoints/closedbook_model.pt
  encoder:
    hidden_size: 256
    dropout: 0.7
    num_layers: 1
    bidirectional: true
  decoder:
    hidden_size: 256 
    dropout: 0.7
    num_layers: 1
    bidirectional: false
# Hyperparameters
  use_gpu: True
  pointer_gen : True
  is_coverage : True
  hidden_dim: 512
  emb_dim: 128
  batch_size: 32
  max_enc_steps: 400
  max_dec_steps: 100
  beam_size: 4
  min_dec_steps: 35
  vocab_size: 50000
  adagrad_init_acc: 0.1
  rand_unif_init_mag: 0.02
  trunc_norm_init_std: 0.0001
  max_grad_norm: 2.0
  device: cuda
  gamma: 0.5
  hidden_size: 512
  dropout: 0.1
training:
  checkpoint_path: "checkpoints"
  learning_rate: 0.001
  score: "rouge"  # Evaluation metric
  patience: 3
  warmup: 1000
  epochs: 10
  batch_size: 32



cov_loss_wt : 1.0
eps : 1e-12
max_iterations : 500000


task: TextSumTask